
import os
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted, ServiceUnavailable
from PIL import Image

class StudyAssistant:
    def __init__(self, api_key):
        genai.configure(api_key=api_key)
        # Using 2.0 Flash as verified available
        self.model = genai.GenerativeModel('gemini-2.0-flash')

    def generate_notes(self, transcript_text, global_context=""):
        """Generates progressive notes (3 levels) in JSON format."""
        import json
        
        # --- MOCK ERROR TRIGGER (For User Testing) ---
        if "SIMULAR_ERROR" in transcript_text:
            raise ResourceExhausted("Simulated Quota Error for Testing")
            
        prompt = f"""
        Act√∫a como un profesor experto. Tu objetivo es crear apuntes en 3 niveles de profundidad (Progresivos) basados en la transcripci√≥n.
        
        CONTEXTO GLOBAL (DEFINICIONES OFICIALES):
        {global_context}
        
        INSTRUCCIONES:
        Genera un objeto JSON estricto con las siguientes claves:
        1. "ultracorto": 5 bullets points con lo esencial (Key takeaways).
        2. "intermedio": 10-12 bullets con los conceptos clave explicados brevemente.
        3. "profundo": Un resumen detallado (aprox 1 p√°gina) con ejemplos, estructura clara, y conectando conceptos con el Contexto Global si aplica.
        
        FORMATO DE SALIDA (JSON √öNICAMENTE - NO PYTHON):
        {{
            "ultracorto": "Texto breve con \\n para saltos...",
            "intermedio": "Texto medio...",
            "profundo": "Texto largo..."
        }}
        (IMPORTANTE: Usa comillas dobles para las claves y valores. Escapa las comillas internas con \\". NO uses triple comilla \"\"\")

        TRANSCRIPCI√ìN:
        {transcript_text} 
        """
        
        try:
            response = self.model.generate_content(prompt)
            text = response.text
            
            # 1. Cleaning
            clean_text = text.replace("```json", "").replace("```", "").strip()
            
            # 2. Extract JSON block if surrounded by text
            import re
            match = re.search(r'\{.*\}', clean_text, re.DOTALL)
            if match:
                clean_text = match.group(0)
            
            # 3. Parsing Strategy
            try:
                return json.loads(clean_text)
            except:
                # Fallback: Try parsing as Python Dictionary (handles triple quotes)
                import ast
                return ast.literal_eval(clean_text)
            
        except ResourceExhausted:
            return {
                "ultracorto": "‚ö†Ô∏è **L√≠mite de IA Excedido**",
                "intermedio": "**¬øPor qu√© veo esto?**\nHas alcanzado el l√≠mite gratuito diario de Google Gemini (Quota Exceeded).",
                "profundo": """### üõë L√≠mite de Tokens Alcanzado
Google ofrece una capa gratuita generosa, pero limitada.

**Soluciones:**
1.  üïí **Esperar:** El cupo se reinicia diariamente. Intenta ma√±ana.
2.  üí≥ **Upgrade:** Configura una tarjeta en Google Cloud Console para permitir "Pay-as-you-go" si necesitas uso intensivo profesional.

*Este mensaje es autom√°tico del sistema de protecci√≥n de costos.*"""
            }
            
        except Exception as e:
            # Fallback for error handling
            return {
                "ultracorto": "Error generando",
                "intermedio": str(e),
                "profundo": response.text if 'response' in locals() else "Error cr√≠tico"
            }

    def generate_study_guide(self, transcript_text, global_context=""):
        """Generates a structured study guide."""
        prompt = f"""
        Act√∫a como un estratega de estudio. Crea una "Gu√≠a de Estudio" basada en esta transcripci√≥n.
        Tu objetivo es que el estudiante apruebe el examen estudiando de forma eficiente.

        CONTEXTO GLOBAL (DEFINICIONES OFICIALES):
        {global_context}
        (Aseg√∫rate de que la estrategia se alinee con estas reglas/definiciones).

        INSTRUCCIONES:
        1. Crea un Mapa de la Unidad (√çndice estructurado).
        2. Lista los Conceptos Clave que seguramente saldr√°n en el examen.
        3. Identifica "Trampas comunes" o errores frecuentes sobre este tema.
        4. Crea un resumen final "En 5 minutos".

        FORMATO DE SALIDA:
        # Gu√≠a de Estudio Estrat√©gica
        ## 1. Mapa de la Unidad
        [Esquema jer√°rquico]
        
        ## 2. Conceptos de Examen
        [Conceptos clave y por qu√© son importantes]
        
        ## 3. Resumen "Si solo tienes 5 minutos"
        [Puntos bala memorables]

        TRANSCRIPCI√ìN:
        {transcript_text}
        """
        response = self.model.generate_content(prompt)
        return response.text

    def solve_quiz(self, images=None, question_text=None, global_context=""):
        """Solves a quiz question from images (list) or text."""
        
        prompt = f"""
        Analiza esta pregunta de examen con el rigor de un CATEDR√ÅTICO UNIVERSITARIO.
        
        CONTEXTO DE LA BIBLIOTECA:
        {global_context}
        
        INSTRUCCIONES SUPREMAS:
        1. **RESOLUCI√ìN DIRECTA**: Identifica la respuesta correcta de inmediato.
        2. **FUENTE DE VERDAD**: 
           - Busca primero en la Biblioteca.
           - Si NO est√° ah√≠, AUTOM√ÅTICAMENTE usa tu conocimiento enciclop√©dico mundial.
           - **PROHIBIDO** decir "El texto no lo dice" o "No tengo informaci√≥n".
           - **PROHIBIDO** ser vago (ej: "√©pocas tempranas"). S√© preciso (ej: "A√±o 1732").
        3. **EXPLICACI√ìN MAGISTRAL (El valor real)**:
           - Tu explicaci√≥n NO debe ser una simple justificaci√≥n. Debe ser una **mini-clase**.
           - Aporta contexto hist√≥rico, define conceptos clave, menciona autores o fechas si aplica.
           - El estudiante debe aprender el "POR QU√â" profundo, no solo cu√°l es la opci√≥n correcta.
           - S√© asertivo, profesional y detallista.

        Salida Estructurada:
        **Pregunta:** [Texto completo]
        **Respuesta Correcta:** [Opci√≥n exacta]
        **Explicaci√≥n:** [P√°rrafo robusto, educativo y detallado que demuestre dominio total del tema]
        """
        
        content_parts = [prompt]
        
        if question_text:
            content_parts.append(f"\nTEXTO DE LA PREGUNTA:\n{question_text}")
            
        if images:
            for img in images:
                content_parts.append(img)
            
        if len(content_parts) == 1: # Only prompt
            return "Error: Por favor proporciona una imagen o escribe el texto de la pregunta."

        
        # Safety catch for None images
        valid_parts = [p for p in content_parts if p is not None]
        
        response = self.model.generate_content(valid_parts)
        return response.text

    def debate_quiz(self, history, latest_input, quiz_context="", images=None):
        """Interacts with user to debate quiz results, seeing the images."""
        
        # Build conversation string
        conv_str = ""
        for msg in history:
            role = "Estudiante" if msg['role'] == "user" else "Profesor"
            conv_str += f"{role}: {msg['content']}\n"
            
        prompt = f"""
        Act√∫a como el Profesor del Diplomado. Est√°s debatiendo los resultados de un examen con el estudiante.
        
        CONTEXTO DEL QUIZ RECIENTE:
        {quiz_context}
        
        HISTORIAL DE CHAT:
        {conv_str}
        
        ESTUDIANTE AHORA: {latest_input}
        
        INSTRUCCIONES:
        1. Tienes acceso visual a las preguntas (Im√°genes) si se adjuntaron. √ösalas para verificar tus respuestas.
        2. S√© amable pero riguroso.
        3. Si el estudiante reclama que una respuesta es incorrecta, ANALIZA su argumento vs la Imagen/Texto.
        4. Si tiene raz√≥n, ADM√çTELO, disc√∫lpate y explica por qu√© la confusi√≥n (quiz√°s ambig√ºedad).
        5. Si no tiene raz√≥n, explica con pedagog√≠a por qu√© la respuesta original es la correcta.
        6. Tu objetivo es que APRENDA, no ganar la discusi√≥n.
        """
        
        content_parts = [prompt]
        
        # Add Images if available to Provide Context
        if images:
            for img in images:
                content_parts.append(img)
                
        # Safety catch
        valid_parts = [p for p in content_parts if p is not None]
        
        response = self.model.generate_content(valid_parts)
        return response.text

    def solve_homework(self, task_prompt, context_texts, task_attachment=None):
        """Solves a homework task using specific library context and optional attachment."""
        
        # Merge all context into one block
        full_context = "\n\n".join(context_texts)
        
        text_prompt = f"""
        Act√∫a como un Asistente Experto del Diplomado.
        Tu misi√≥n es ayudar al estudiante a realizar su tarea PERFECTAMENTE, bas√°ndote en la metodolog√≠a oficial del curso.
        
        CONTEXTO OFICIAL (BIBLIOTECA):
        A continuaci√≥n tienes la informaci√≥n extra√≠da directamente de las clases. USALA COMO TU PRIMERA FUENTE DE VERDAD.
        ------------------------------------------------------------
        {full_context}
        ------------------------------------------------------------
        
        TAREA DEL USUARIO:
        {task_prompt}
        
        (Si hay un archivo adjunto, contiene los detalles espec√≠ficos o el PDF de la consigna. √ösalo como gu√≠a principal para la estructura de la tarea).
        
        INSTRUCCIONES:
        1. Analiza la tarea y busca relaciones directas en el Contexto Oficial.
        2. Si el contexto menciona un m√©todo paso a paso, √öSALO.
        3. No inventes metodolog√≠as externas si el curso ya provee una.
        4. Si falta informaci√≥n en el contexto, usa tu conocimiento general pero advierte: "Nota: Esto no estaba en tus apuntes, as√≠ que uso conocimiento general".
        5. Se pr√°ctico, directo y organizado.
        
        RESPUESTA SOLICITADA:
        [Desarrolla la tarea o da la gu√≠a paso a paso basada en el material]
        
        IMPORTANTE:
        - Si la respuesta est√° en el CONTEXTO OFICIAL, √∫salo obligatoriamente.
        - Si el usuario te pide algo que contradice el contexto oficial, explica la discrepancia.
        - No digas "necesito m√°s contexto" si el usuario ya te dio archivos adjuntos o si hay texto en la biblioteca. Haz tu mejor esfuerzo con lo que hay.
        """
        
        content_parts = [text_prompt]
        
        if task_attachment:
            # task_attachment is expected to be a dict: {"mime_type": str, "data": bytes}
            # OR a PIL Image if strictly image.
            # But for PDF/General support via Gemini API, we pass the blob.
            import io
             # If it's a PIL Image (legacy flow), convert to blob? 
             # Let's assume input is raw bytes and mime_type from streamlit.
            
            # Helper to create the part object for google-generativeai
            blob = {
                "mime_type": task_attachment['mime_type'],
                "data": task_attachment['data']
            }
            content_parts.append(blob)
        
        response = self.model.generate_content(content_parts)
        return response.text

    def extract_text_from_pdf(self, pdf_data, mime_type="application/pdf"):
        """Extracts text from a PDF using Gemini (High Quality OCR/Layout analysis)."""
        prompt = """
        Extract ALL text from this document verbatim. 
        Preserve the structure (headers, lists) using Markdown.
        Do not summarize. Just output the full content.
        """
        
        blob = {"mime_type": mime_type, "data": pdf_data}
        try:
            response = self.model.generate_content([prompt, blob])
            return response.text
        except Exception as e:
            return f"Error reading PDF: {e}"

    def search_knowledge_base(self, query, context, mode="Concepto R√°pido"):
        """Searches the knowledge base with specific mode."""
        
        if "R√°pido" in mode:
            prompt = f"""
            Act√∫a como un Buscador Acad√©mico de Alta Precisi√≥n (Tipo Spotlight).
            
            CONSULTA: "{query}"
            
            CONTEXTO GENERAL (Toda la Bibliograf√≠a):
            --------------------------------------------------
            {context}
            --------------------------------------------------
            
            OBJETIVO:
            Da una definici√≥n DIRECTA y CONCISA.
            Cita expl√≠citamente el archivo o video de donde sale la informaci√≥n.
            Si no est√° en el contexto, dilo claramente.
            
            FORMATO:
            **Definici√≥n:** [Explicaci√≥n breve]
            **Fuente:** [Nombre del archivo/video exacto]
            """
        else:
            prompt = f"""
            Act√∫a como un Investigador Acad√©mico Senior.
            
            CONSULTA: "{query}"
            
            CONTEXTO GENERAL (Toda la Bibliograf√≠a):
            --------------------------------------------------
            {context}
            --------------------------------------------------
            
            OBJETIVO:
            Realiza un an√°lisis profundo conectando puntos entre diferentes clases/archivos.
            Explica la relaci√≥n entre conceptos si es necesario.
            Sintetiza la respuesta como un experto.
            
            FORMATO:
            **An√°lisis Sintetizado:**
            [Respuesta detallada y explicada]
            
            **Fuentes Consultadas:**
            - [Archivo 1]
            - [Archivo 2]
            """

        response = self.model.generate_content(prompt)
        return response.text

    def solve_argumentative_task(self, task_prompt, context_files=[], global_context=""):
        """Solves complex tasks with a structured 4-part response (JSON)."""
        import json
        
        # Build Context
        context_str = global_context
        if context_files:
            context_str += "\n\n--- DOCUMENTOS ADJUNTOS ---\n"
            for f in context_files:
                context_str += f"[NOMBRE: {f['name']}]\n{f['content']}\n\n"
        
        prompt = f"""
        Act√∫a como un CONSULTOR EXPERTO y ABOGADO DEL DIABLO. Tu misi√≥n es resolver la siguiente tarea acad√©mica compleja con un nivel de an√°lisis profundo.
        
        CONTEXTO / FUENTES:
        {context_str}
        
        TAREA DEL USUARIO:
        {task_prompt}
        
        INSTRUCCIONES CLAVE:
        1. Analiza el problema desde m√∫ltiples √°ngulos.
        2. Usa las fuentes proporcionadas expl√≠citamente SI EXISTEN.
        3. Si NO hay fuentes o faltan datos, usa tu CONOCIMIENTO GENERAL EXPERTO para resolverlo, pero aclara que es informaci√≥n externa.
        4. Anticipa cr√≠ticas o fallos en tu propio razonamiento (Contra-argumento).
        
        FORMATO DE SALIDA (JSON ESTRICTO):
        Debes devolver un JSON con estas 4 claves exactas:
        1. "direct_response": La respuesta final pulida, lista para entregar. (Markdown).
        2. "sources": Lista de archivos/conceptos espec√≠ficos de la biblioteca que usaste. (Markdown).
        3. "step_by_step": Tu proceso l√≥gico de deducci√≥n para llegar a la respuesta. (Markdown).
        4. "counter_argument": Objeciones s√≥lidas a tu propia respuesta (Abogado del diablo). (Markdown).
        
        JSON:
        {{
            "direct_response": "...",
            "sources": "...",
            "step_by_step": "...",
            "counter_argument": "..."
        }}
        """
        
        try:
            response = self.model.generate_content(prompt)
            clean_text = response.text.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_text)
        except Exception as e:
            return {
                "direct_response": "Error generando respuesta estructurada.",
                "sources": "N/A",
                "step_by_step": str(e),
                "counter_argument": "No se pudo generar."
            }

    def chat_tutor(self, current_user_msg, chat_history=[], context_files=[], global_context=""):
        """
        Conversational Tutor that remembers history and uses context.
        chat_history: List of dicts {'role': 'user'/'model', 'content': '...'}
        """
        
        # Build Context String
        context_str = global_context
        if context_files:
            context_str += "\n\n--- ARCHIVOS ADJUNTOS EN ESTE MENSAJE ---\n"
            for f in context_files:
                context_str += f"[NOMBRE: {f['name']}]\n{f['content']}\n\n"
        
        # Construct System Prompt / History for Gemini
        # We can use the chat API or a single prompt with history injection.
        # For simplicity and control over context, prompt injection is often more robust for "persona" maintenance.
        
        system_instruction = f"""
        Act√∫a como un PROFESOR UNIVERSITARIO PROACTIVO Y SAPIENTE.
        Tu nombre es "Profe. IA".
        
        CONTEXTO DE CONOCIMIENTO (BIBLIOTECA):
        {context_str}
        (Usa esta informaci√≥n para responder, corregir o proponer ejemplos).
        
        INSTRUCCIONES:
        1. Mant√©n un tono acad√©mico pero cercano.
        2. Si el alumno te saluda, saluda y pregunta qu√© est√° estudiando hoy.
        3. Si te env√≠a una tarea, CORR√çGELA con rigor y explica los errores.
        4. Si te hace una pregunta, resp√≥ndela conectando conceptos de la biblioteca.
        5. SIEMPRE termina fomentando el pensamiento cr√≠tico con una pregunta de vuelta.
        
        HISTORIAL DE CONVERSACI√ìN:
        """
        
        # simple history flattener
        history_str = ""
        for msg in chat_history[-10:]: # Keep last 10 turns context window for efficiency
             role = "ALUMNO" if msg['role'] == "user" else "PROFESOR"
             history_str += f"{role}: {msg['content']}\n"
             
        final_prompt = f"{system_instruction}\n{history_str}\nALUMNO: {current_user_msg}\nPROFESOR:"
        
        try:
            response = self.model.generate_content(final_prompt)
            return response.text
        except Exception as e:
            return f"Error en la clase: {str(e)}"

    def process_bulk_chat(self, raw_text, user_instructions=""):
        # ... (Existing logic kept for fallback or specific manual triggers if needed) ...
        # (Actually, we might repurpose this heavily, but for now lets add the NEW flexible methods)
        pass

    def analyze_import_file(self, raw_text):
        """
        Generates a high-level summary of the file to start the conversation.
        """
        snippet = raw_text[:10000] # Analyze first 10k chars for speed + random sample if needed
        prompt = f"""
        Act√∫a como un Asistente de Archivos Inteligente.
        Acabas de recibir este archivo de texto (Chat exportado o apuntes).
        
        Tu misi√≥n: Dar un resumen brev√≠simo de qu√© contiene para preguntarle al usuario qu√© hacer.
        
        FRAGMENTO (Primeros caracteres):
        {snippet}
        ...
        
        SALIDA ESPERADA (Solo texto, tono amable y servicial):
        "Hola! He le√≠do tu archivo. Parece contener [X, Y, Z]. Veo fechas de [Tema] y apuntes sobre [Tema]. ¬øC√≥mo quieres que lo organice?"
        """
        response = self.model.generate_content(prompt)
        return response.text

    def chat_with_import_file(self, raw_text, user_message, chat_history, available_folders=[]):
        """
        The core logic for the Import Assistant.
        Decides whether to reply to the user OR generate a JSON Action to modify the DB.
        """
        import json
        
        # Build prompt
        folders_str = ", ".join([f['name'] for f in available_folders])
        
        history_text = ""
        for msg in chat_history[-6:]:
            role = "USUARIO" if msg['role'] == "user" else "ASISTENTE"
            history_text += f"{role}: {msg['content']}\n"
        
        snippet = raw_text[:20000] # Context window limit
        
        prompt = f"""
        ERES UN GESTOR DE ARCHIVOS INTELIGENTE (IMPORT ASSISTANT) - MODO SOCR√ÅTICO Y PROFUNDO.
        Est√°s conversando con el usuario para organizar este archivo en su Biblioteca.
        
        TU PERSONALIDAD:
        1.  **PROFUNDO Y EXTENSO**: Odias las respuestas cortas. Cuando expliques algo, hazlo con detalle, ejemplos y matices.
        2.  **SOCR√ÅTICO**: No solo obedezcas. **Haz preguntas** si algo es ambiguo. Ayuda al usuario a pensar mejor.
        3.  **EXPL√çCITO**: Si resumes, no digas "aqu√≠ hay datos". Di "El documento detalla X, Y, Z, con √©nfasis en A y B".
        4.  **ESTRUCTURADO**: Siempre busca la mejor manera de dividir la informaci√≥n en m√∫ltiples archivos l√≥gicos.
        
        INSTRUCCIONES CLAVE (MODO EXPERTO):
        1. Eres un arquitecto de informaci√≥n. Tu objetivo es ESTRUCTURAR el contenido.
        2. Puedes ejecutar M√öLTIPLES acciones en una sola respuesta.
        3. SIEMPRE usa formato JSON para acciones (guardar, crear carpetas).
        4. Si el usuario pide "Saca el resumen y las fechas", crea DOS archivos separados en el mismo turno.
        5. **NUNCA seas superficial.** Si generas un resumen, que sea ROBUSTO.
        
        FORMATO DE ACCI√ìN (JSON OBLIGATORIO PARA COMANDOS):
        {{
            "thoughts": "Breve razonamiento de qu√© vas a hacer...",
            "actions": [
                {{
                    "action_type": "save_file",
                    "target_folder": "Nombre Carpeta",
                    "file_name": "Resumen.md",
                    "content": "..."
                }},
                {{
                    "action_type": "save_file",
                    "target_folder": "Nombre Carpeta",
                    "file_name": "Fechas.md",
                    "content": "..."
                }}
            ]
        }}
        
        SI ES SOLO CONVERSACI√ìN:
        Simplemente responde con texto plano.
        
        ARCHIVO (Contexto):
        {snippet}
        ...
        
        HISTORIAL:
        {history_text}
        USUARIO: {user_message}
        ASISTENTE (JSON 'actions' o Texto):
        """
        
        try:
             response = self.model.generate_content(prompt)
             txt = response.text.strip()
             
             # Robust Parsing: Find first { and last }
             import re
             json_match = re.search(r"\{.*\}", txt, re.DOTALL)
             
             if json_match:
                 try:
                     clean_json = json_match.group(0)
                     return json.loads(clean_json)
                 except:
                     pass # Fallback to text if malformed
             
             return txt
        except Exception as e:
            return f"Error pensando: {e}"


